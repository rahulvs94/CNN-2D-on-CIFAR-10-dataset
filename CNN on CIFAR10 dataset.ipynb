{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 Dataset link: \n",
    "http://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split datas between train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (50000, 32, 32, 3)\n",
      "x_train samples:  50000\n",
      "y_train samples:  (50000, 1)\n",
      "\n",
      "nx_test shape:  (10000, 32, 32, 3)\n",
      "x_test samples:  50000\n",
      "y_test samples:  (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_train samples: ', x_train.shape[0])\n",
    "print('y_train samples: ', y_train.shape)\n",
    "print('\\nnx_test shape: ', x_test.shape)\n",
    "print('x_test samples: ', x_train.shape[0])\n",
    "print('y_test samples: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert classes vectors to binary class matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First y_train sample:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('First y_train sample: ', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(LeakyReLU(alpha=.001))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(LeakyReLU(alpha=.001))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(LeakyReLU(alpha=.001))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(LeakyReLU(alpha=.001))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(LeakyReLU(alpha=.001))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,513,514\n",
      "Trainable params: 1,513,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.3, batch_size=batch_size, epochs=epochs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting loss vs epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate test samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ReLU activation function, Validation accuracy = 75.67%, Train accuracy = 78.84%, Test accuracy = 75.13%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overall, model has high bias and high variance and required more than 3 hrs to train the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Leaky ReLU activation function, Validation accuracy = 74.92%, Train accuracy = 79.21%, Test accuracy = 74.98%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After changing the activation function to Leaky ReLU, model still has high bias and high variance. By inspection, this model required more time than model with ReLU activation to train the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will check the behaviour of model after feeding scaled training and test samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling test and training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.3, batch_size=batch_size, epochs=epochs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting loss vs epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate test samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy = 79.07%, Train accuracy = 84.43%, Test accuracy = 78.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, Analyzing this model will show that training error is around 15% whereas validation error is 21%. If we compare with model without scaling the dataset, then test accuracy has increased by 3%. Bias has been decreased but variance is high. Next step will be to reduce high variance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To reduce the high variance problem, we can add more data by data augmentation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will check the behaviour of model after feeding scaled training and test samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n"
     ]
    }
   ],
   "source": [
    "print('Using real-time data augmentation.')\n",
    "\n",
    "# This will do preprocessing and realtime data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    # set input mean to 0 over the dataset\n",
    "    featurewise_center=False, \n",
    "    # set each sample mean to 0\n",
    "    samplewise_center=False,  \n",
    "    # divide inputs by std of the dataset\n",
    "    featurewise_std_normalization=False, \n",
    "    # divide each input by its std\n",
    "    samplewise_std_normalization=False,\n",
    "    # apply ZCA whitening\n",
    "    zca_whitening=False,  \n",
    "    # epsilon for ZCA whitening\n",
    "    zca_epsilon=1e-06,  \n",
    "    # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    rotation_range=0,  \n",
    "    # randomly shift images horizontally (fraction of total width)    \n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically (fraction of total height)\n",
    "    height_shift_range=0.1,\n",
    "    # set range for random shear\n",
    "    shear_range=0., \n",
    "    # set range for random zoom\n",
    "    zoom_range=0.,  \n",
    "    # set range for random channel shifts\n",
    "    channel_shift_range=0.,  \n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    # value used for fill_mode = \"constant\"\n",
    "    cval=0.,  \n",
    "    # randomly flip images\n",
    "    horizontal_flip=True,  \n",
    "    # randomly flip images\n",
    "    vertical_flip=False,  \n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute quantities required for feature-wise normalization\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model on the batches generated by datagen.flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "500/500 [==============================] - 302s 604ms/step - loss: 1.9693 - acc: 0.2736\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 291s 583ms/step - loss: 1.7137 - acc: 0.3748\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 284s 568ms/step - loss: 1.6128 - acc: 0.4089\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 277s 555ms/step - loss: 1.5341 - acc: 0.4424\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 267s 534ms/step - loss: 1.4725 - acc: 0.4651\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 269s 538ms/step - loss: 1.4198 - acc: 0.4868\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 267s 533ms/step - loss: 1.3767 - acc: 0.5036\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 265s 529ms/step - loss: 1.3426 - acc: 0.5180\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 264s 529ms/step - loss: 1.3010 - acc: 0.5334\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 265s 529ms/step - loss: 1.2737 - acc: 0.5443\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 265s 531ms/step - loss: 1.2432 - acc: 0.5592\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 264s 527ms/step - loss: 1.2152 - acc: 0.5670\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 268s 535ms/step - loss: 1.1868 - acc: 0.5785\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 264s 527ms/step - loss: 1.1661 - acc: 0.5858\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 263s 526ms/step - loss: 1.1485 - acc: 0.5940\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 266s 533ms/step - loss: 1.1259 - acc: 0.6020\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 268s 535ms/step - loss: 1.1130 - acc: 0.6064\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 264s 527ms/step - loss: 1.0882 - acc: 0.6156\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 263s 526ms/step - loss: 1.0770 - acc: 0.6196\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 263s 526ms/step - loss: 1.0650 - acc: 0.6249\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 264s 529ms/step - loss: 1.0438 - acc: 0.6349\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 263s 527ms/step - loss: 1.0321 - acc: 0.6353\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 263s 525ms/step - loss: 1.0211 - acc: 0.6391\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 263s 525ms/step - loss: 1.0132 - acc: 0.6425\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 264s 529ms/step - loss: 0.9986 - acc: 0.6485\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 265s 531ms/step - loss: 0.9898 - acc: 0.6513\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 270s 540ms/step - loss: 0.9774 - acc: 0.6568\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 264s 528ms/step - loss: 0.9652 - acc: 0.6592\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 263s 526ms/step - loss: 0.9586 - acc: 0.6612\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 263s 527ms/step - loss: 0.9455 - acc: 0.6684\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 263s 527ms/step - loss: 0.9400 - acc: 0.6680\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 264s 528ms/step - loss: 0.9327 - acc: 0.6721\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 264s 528ms/step - loss: 0.9245 - acc: 0.6758\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 266s 532ms/step - loss: 0.9107 - acc: 0.6807\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 264s 529ms/step - loss: 0.9039 - acc: 0.6838\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 264s 527ms/step - loss: 0.9015 - acc: 0.6837\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 265s 529ms/step - loss: 0.8918 - acc: 0.6867\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 264s 528ms/step - loss: 0.8859 - acc: 0.6863\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 269s 538ms/step - loss: 0.8746 - acc: 0.6924\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 268s 536ms/step - loss: 0.8663 - acc: 0.6949\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 264s 528ms/step - loss: 0.8640 - acc: 0.6975\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 265s 530ms/step - loss: 0.8581 - acc: 0.6981\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 264s 529ms/step - loss: 0.8540 - acc: 0.7032\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 264s 528ms/step - loss: 0.8472 - acc: 0.7033\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 265s 530ms/step - loss: 0.8372 - acc: 0.7061\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 264s 528ms/step - loss: 0.8334 - acc: 0.7104\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 265s 529ms/step - loss: 0.8292 - acc: 0.7113\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 267s 534ms/step - loss: 0.8209 - acc: 0.7138\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 268s 535ms/step - loss: 0.8236 - acc: 0.7135\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 268s 536ms/step - loss: 0.8118 - acc: 0.7185\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 267s 533ms/step - loss: 0.8020 - acc: 0.7208\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 269s 538ms/step - loss: 0.7984 - acc: 0.7209\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 270s 539ms/step - loss: 0.7995 - acc: 0.7203\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 265s 529ms/step - loss: 0.7995 - acc: 0.7227\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 266s 533ms/step - loss: 0.7890 - acc: 0.7265\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 265s 531ms/step - loss: 0.7858 - acc: 0.7255\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 267s 533ms/step - loss: 0.7829 - acc: 0.7283\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 266s 532ms/step - loss: 0.7785 - acc: 0.7294\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 269s 538ms/step - loss: 0.7739 - acc: 0.7306\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 269s 538ms/step - loss: 0.7738 - acc: 0.7318\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 271s 542ms/step - loss: 0.7691 - acc: 0.7320\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 274s 548ms/step - loss: 0.7705 - acc: 0.7338\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 270s 540ms/step - loss: 0.7650 - acc: 0.7359\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 266s 533ms/step - loss: 0.7613 - acc: 0.7370\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 266s 531ms/step - loss: 0.7565 - acc: 0.7381\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 265s 530ms/step - loss: 0.7525 - acc: 0.7408\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 266s 533ms/step - loss: 0.7515 - acc: 0.7423\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 265s 530ms/step - loss: 0.7502 - acc: 0.7395\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 267s 533ms/step - loss: 0.7503 - acc: 0.7430\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 265s 530ms/step - loss: 0.7451 - acc: 0.7424\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 265s 530ms/step - loss: 0.7441 - acc: 0.7428\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 267s 533ms/step - loss: 0.7420 - acc: 0.7447\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 266s 532ms/step - loss: 0.7310 - acc: 0.7467\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 265s 531ms/step - loss: 0.7375 - acc: 0.7472\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 265s 530ms/step - loss: 0.7341 - acc: 0.7481\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 266s 532ms/step - loss: 0.7296 - acc: 0.7481\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 267s 534ms/step - loss: 0.7284 - acc: 0.7505\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 265s 531ms/step - loss: 0.7270 - acc: 0.7492\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 266s 532ms/step - loss: 0.7230 - acc: 0.7514\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 269s 539ms/step - loss: 0.7206 - acc: 0.7520\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 272s 543ms/step - loss: 0.7198 - acc: 0.7527\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 267s 535ms/step - loss: 0.7224 - acc: 0.7503\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 267s 534ms/step - loss: 0.7220 - acc: 0.7519\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 271s 541ms/step - loss: 0.7110 - acc: 0.7525\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 266s 531ms/step - loss: 0.7139 - acc: 0.7543\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 262s 525ms/step - loss: 0.7150 - acc: 0.7560\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 263s 526ms/step - loss: 0.7094 - acc: 0.7592\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 264s 528ms/step - loss: 0.7132 - acc: 0.7555\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 263s 526ms/step - loss: 0.7112 - acc: 0.7578\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 262s 524ms/step - loss: 0.7060 - acc: 0.7581\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 262s 524ms/step - loss: 0.7024 - acc: 0.7618\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 266s 533ms/step - loss: 0.7044 - acc: 0.7598\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 266s 532ms/step - loss: 0.7086 - acc: 0.7596\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 269s 538ms/step - loss: 0.7041 - acc: 0.7612\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 268s 537ms/step - loss: 0.7002 - acc: 0.7622\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 267s 534ms/step - loss: 0.6985 - acc: 0.7596\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 263s 525ms/step - loss: 0.6955 - acc: 0.7641\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 262s 525ms/step - loss: 0.6960 - acc: 0.7632\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 263s 526ms/step - loss: 0.6943 - acc: 0.7620\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 263s 526ms/step - loss: 0.7008 - acc: 0.7598\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), epochs=epochs, validation_data=None, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting loss vs epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCiIi0kmhICIinf4/WqXLhkGk4mMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate test samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 21s 2ms/step\n",
      "Test loss: 0.6223537016868591\n",
      "Test accuracy: 0.788\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracy = 75.98%, Test accuracy = 78.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we compare the train accuracy with model applied after scaled data, then it has decreased by 8% and test accuracy has increased by just 0.5%. We can see that model still has high bias problem and adding the data to train the model has not helped in this case. We can try to add the features or train with different model to check the performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
