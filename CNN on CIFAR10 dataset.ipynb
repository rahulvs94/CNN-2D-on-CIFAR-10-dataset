{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 Dataset link: \n",
    "http://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers.advanced_activations import ELU\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "# opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split datas between train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (50000, 32, 32, 3)\n",
      "x_train samples:  50000\n",
      "y_train samples:  (50000, 1)\n",
      "\n",
      "nx_test shape:  (10000, 32, 32, 3)\n",
      "x_test samples:  50000\n",
      "y_test samples:  (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_train samples: ', x_train.shape[0])\n",
    "print('y_train samples: ', y_train.shape)\n",
    "print('\\nnx_test shape: ', x_test.shape)\n",
    "print('x_test samples: ', x_train.shape[0])\n",
    "print('y_test samples: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert classes vectors to binary class matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First y_train sample:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('First y_train sample: ', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "# model.add(Activation('relu'))\n",
    "model.add(ELU(alpha=1.0))\n",
    "# model.add(LeakyReLU(alpha=.001))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "model.add(ELU(alpha=1.0))\n",
    "# model.add(LeakyReLU(alpha=.001))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "model.add(ELU(alpha=1.0))\n",
    "# model.add(LeakyReLU(alpha=.001))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "model.add(ELU(alpha=1.0))\n",
    "# model.add(LeakyReLU(alpha=.001))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "# model.add(Activation('relu'))\n",
    "model.add(ELU(alpha=1.0))\n",
    "# model.add(LeakyReLU(alpha=.001))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "elu_1 (ELU)                  (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "elu_2 (ELU)                  (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "elu_3 (ELU)                  (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "elu_4 (ELU)                  (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "elu_5 (ELU)                  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.3, batch_size=batch_size, epochs=epochs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting loss vs epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate test samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ReLU activation function, Validation accuracy = 75.67%, Train accuracy = 78.84%, Test accuracy = 75.13%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overall, model has high bias and high variance and required more than 3 hrs to train the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Leaky ReLU activation function, Validation accuracy = 74.92%, Train accuracy = 79.21%, Test accuracy = 74.98%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After changing the activation function to Leaky ReLU, model still has high bias and high variance. By inspection, this model required more time than model with ReLU activation to train the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will check the behaviour of model after feeding scaled training and test samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling test and training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.3, batch_size=batch_size, epochs=epochs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting loss vs epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate test samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy = 79.07%, Train accuracy = 84.43%, Test accuracy = 78.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, Analyzing this model will show that training error is around 15% whereas validation error is 21%. If we compare with model without scaling the dataset, then test accuracy has increased by 3%. Bias has been decreased but variance is high. Next step will be to reduce high variance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To reduce the high variance problem, we can add more data by data augmentation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will check the behaviour of model after feeding scaled training and test samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n"
     ]
    }
   ],
   "source": [
    "print('Using real-time data augmentation.')\n",
    "\n",
    "# This will do preprocessing and realtime data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    # set input mean to 0 over the dataset\n",
    "    featurewise_center=False, \n",
    "    # set each sample mean to 0\n",
    "    samplewise_center=False,  \n",
    "    # divide inputs by std of the dataset\n",
    "    featurewise_std_normalization=False, \n",
    "    # divide each input by its std\n",
    "    samplewise_std_normalization=False,\n",
    "    # apply ZCA whitening\n",
    "    zca_whitening=False,  \n",
    "    # epsilon for ZCA whitening\n",
    "    zca_epsilon=1e-06,  \n",
    "    # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    rotation_range=0,  \n",
    "    # randomly shift images horizontally (fraction of total width)    \n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically (fraction of total height)\n",
    "    height_shift_range=0.1,\n",
    "    # set range for random shear\n",
    "    shear_range=0., \n",
    "    # set range for random zoom\n",
    "    zoom_range=0.,  \n",
    "    # set range for random channel shifts\n",
    "    channel_shift_range=0.,  \n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    # value used for fill_mode = \"constant\"\n",
    "    cval=0.,  \n",
    "    # randomly flip images\n",
    "    horizontal_flip=True,  \n",
    "    # randomly flip images\n",
    "    vertical_flip=False,  \n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute quantities required for feature-wise normalization\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model on the batches generated by datagen.flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "500/500 [==============================] - 310s 619ms/step - loss: 1.6150 - acc: 0.4225\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 298s 596ms/step - loss: 1.3475 - acc: 0.5290\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 306s 611ms/step - loss: 1.2638 - acc: 0.5594\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 293s 586ms/step - loss: 1.2001 - acc: 0.5813\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 284s 568ms/step - loss: 1.1495 - acc: 0.5969\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 286s 573ms/step - loss: 1.0982 - acc: 0.6164\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 295s 591ms/step - loss: 1.0611 - acc: 0.6293\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 321s 642ms/step - loss: 1.0296 - acc: 0.6392\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 315s 630ms/step - loss: 1.0008 - acc: 0.6510\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 316s 633ms/step - loss: 0.9775 - acc: 0.6595\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 313s 625ms/step - loss: 0.9551 - acc: 0.6679\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 299s 598ms/step - loss: 0.9459 - acc: 0.6717\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 307s 613ms/step - loss: 0.9273 - acc: 0.6786\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 317s 634ms/step - loss: 0.9113 - acc: 0.6845\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 324s 647ms/step - loss: 0.8983 - acc: 0.6888\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 302s 604ms/step - loss: 0.8898 - acc: 0.6906\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 317s 633ms/step - loss: 0.8817 - acc: 0.6939\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 300s 600ms/step - loss: 0.8763 - acc: 0.6991\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 299s 598ms/step - loss: 0.8698 - acc: 0.6994\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 281s 561ms/step - loss: 0.8521 - acc: 0.7080\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 293s 587ms/step - loss: 0.8452 - acc: 0.7092\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 291s 583ms/step - loss: 0.8394 - acc: 0.7105\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 297s 594ms/step - loss: 0.8355 - acc: 0.7127\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 310s 621ms/step - loss: 0.8355 - acc: 0.7142\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 305s 609ms/step - loss: 0.8200 - acc: 0.7188\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 285s 570ms/step - loss: 0.8208 - acc: 0.7197\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 281s 563ms/step - loss: 0.8195 - acc: 0.7198\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 280s 560ms/step - loss: 0.8127 - acc: 0.7211\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 278s 555ms/step - loss: 0.8092 - acc: 0.7225\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 274s 549ms/step - loss: 0.8017 - acc: 0.7258\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 273s 546ms/step - loss: 0.7973 - acc: 0.7251\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 273s 546ms/step - loss: 0.7963 - acc: 0.7266\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 273s 547ms/step - loss: 0.7916 - acc: 0.7289\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 273s 545ms/step - loss: 0.7915 - acc: 0.7299\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 278s 556ms/step - loss: 0.7815 - acc: 0.7332\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 277s 554ms/step - loss: 0.7755 - acc: 0.7350\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 273s 545ms/step - loss: 0.7816 - acc: 0.7338\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 273s 547ms/step - loss: 0.7762 - acc: 0.7341\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 273s 547ms/step - loss: 0.7721 - acc: 0.7370\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 273s 546ms/step - loss: 0.7781 - acc: 0.7348\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 273s 546ms/step - loss: 0.7636 - acc: 0.7401\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 273s 547ms/step - loss: 0.7651 - acc: 0.7386\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 273s 546ms/step - loss: 0.7691 - acc: 0.7394\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 273s 547ms/step - loss: 0.7671 - acc: 0.7399\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 278s 556ms/step - loss: 0.7567 - acc: 0.7425\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 273s 546ms/step - loss: 0.7589 - acc: 0.7437\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 274s 547ms/step - loss: 0.7532 - acc: 0.7448\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 275s 551ms/step - loss: 0.7509 - acc: 0.7443\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 282s 563ms/step - loss: 0.7506 - acc: 0.7469\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 278s 555ms/step - loss: 0.7466 - acc: 0.7483\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 277s 554ms/step - loss: 0.7426 - acc: 0.7483\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 280s 560ms/step - loss: 0.7474 - acc: 0.7465\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 277s 553ms/step - loss: 0.7423 - acc: 0.7502\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 275s 549ms/step - loss: 0.7411 - acc: 0.7497\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 273s 547ms/step - loss: 0.7362 - acc: 0.7510\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 274s 548ms/step - loss: 0.7344 - acc: 0.7537\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 279s 558ms/step - loss: 0.7387 - acc: 0.7499\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 275s 550ms/step - loss: 0.7317 - acc: 0.7532\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 274s 549ms/step - loss: 0.7414 - acc: 0.7502\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 275s 550ms/step - loss: 0.7320 - acc: 0.7534\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 278s 556ms/step - loss: 0.7272 - acc: 0.7551\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 279s 557ms/step - loss: 0.7212 - acc: 0.7554\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 280s 560ms/step - loss: 0.7286 - acc: 0.7532\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 273s 547ms/step - loss: 0.7317 - acc: 0.7529\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 273s 547ms/step - loss: 0.7268 - acc: 0.7543\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 273s 546ms/step - loss: 0.7188 - acc: 0.7578\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 273s 546ms/step - loss: 0.7150 - acc: 0.7589\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 273s 547ms/step - loss: 0.7113 - acc: 0.7619\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 273s 546ms/step - loss: 0.7106 - acc: 0.7614\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 274s 548ms/step - loss: 0.7109 - acc: 0.7597\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 273s 547ms/step - loss: 0.7144 - acc: 0.7596\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 274s 548ms/step - loss: 0.7143 - acc: 0.7596\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 275s 550ms/step - loss: 0.7228 - acc: 0.7578\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 275s 551ms/step - loss: 0.7055 - acc: 0.7618\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 273s 547ms/step - loss: 0.7171 - acc: 0.7605\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 275s 549ms/step - loss: 0.7034 - acc: 0.7645\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 274s 548ms/step - loss: 0.7051 - acc: 0.7642\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 273s 547ms/step - loss: 0.7107 - acc: 0.7617\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 281s 561ms/step - loss: 0.7035 - acc: 0.7672\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 274s 548ms/step - loss: 0.7066 - acc: 0.7624\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 274s 547ms/step - loss: 0.7154 - acc: 0.7614\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 271s 541ms/step - loss: 0.7036 - acc: 0.7660\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 271s 542ms/step - loss: 0.6979 - acc: 0.7668\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 271s 541ms/step - loss: 0.7001 - acc: 0.7672\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 271s 542ms/step - loss: 0.6960 - acc: 0.7670\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 270s 540ms/step - loss: 0.7007 - acc: 0.7670\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 273s 545ms/step - loss: 0.6983 - acc: 0.7664\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 270s 541ms/step - loss: 0.6978 - acc: 0.7703\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 276s 553ms/step - loss: 0.6949 - acc: 0.7676\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 271s 542ms/step - loss: 0.6969 - acc: 0.7674\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 271s 541ms/step - loss: 0.6854 - acc: 0.7702\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 271s 541ms/step - loss: 0.6889 - acc: 0.7706\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 271s 542ms/step - loss: 0.6893 - acc: 0.7694\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 4903s 10s/step - loss: 0.6879 - acc: 0.7699\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 277s 554ms/step - loss: 0.6880 - acc: 0.7719\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 274s 549ms/step - loss: 0.6932 - acc: 0.7698\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 274s 547ms/step - loss: 0.6888 - acc: 0.7717\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 272s 544ms/step - loss: 0.6904 - acc: 0.7713\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 272s 544ms/step - loss: 0.6894 - acc: 0.7709\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 274s 548ms/step - loss: 0.6886 - acc: 0.7722\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), epochs=epochs, validation_data=None, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting loss vs epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate test samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 21s 2ms/step\n",
      "Test loss: 0.5238166380882263\n",
      "Test accuracy: 0.8274\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracy = 75.98%, Test accuracy = 78.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we compare the train accuracy with model applied after scaled data, then it has decreased by 8% and test accuracy has increased by just 0.5%. We can see that model still has high bias problem and adding the data to train the model has not helped in this case. We can try,** \n",
    "1. add the features\n",
    "2. train for longer time\n",
    "3. try with different optimizer\n",
    "4. train with different model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PART 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will se the performace after changing the optimizer from RMSProp to Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracy = 81.24%, Test accuracy = 82.67%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we compare accuracy with previous trained model, then train accuracy has increased by 5% and test accuracy has increased by almost 4%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's chnage the activation function. According to researchers, exponential linear function (ELU) works faster compare to LReLU and ReLU activation functions. It helps better the performance but increase runtime for training data. I have added this function in model definition itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracy = 77.22%, Test accuracy = 82.74%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Working with different activation functions made no serious change in the performance. Since, we are facing high bias issue, we should consider training the complex model or replacing the model.\n",
    "Here is the link for reference: https://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
